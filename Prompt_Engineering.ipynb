{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNFVHfI5PR2h"
   },
   "source": [
    "Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHyvryXwGmTP",
    "outputId": "bdb258d9-2a96-491f-bf86-42fdfe6bbca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-generativeai\n",
      "Version: 0.5.4\n",
      "Summary: Google Generative AI High level API client library and tools.\n",
      "Home-page: https://github.com/google/generative-ai-python\n",
      "Author: Google LLC\n",
      "Author-email: googleapis-packages@google.com\n",
      "License: Apache 2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: google-ai-generativelanguage, google-api-core, google-api-python-client, google-auth, protobuf, pydantic, tqdm, typing-extensions\n",
      "Required-by: langchain-google-genai\n",
      "Name: langchain-google-genai\n",
      "Version: 1.0.6\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: google-generativeai, langchain-core\n",
      "Required-by: \n",
      "Name: langchain\n",
      "Version: 0.2.5\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: aiohttp, async-timeout, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-generativeai\n",
    "!pip install -q langchain-google-genai\n",
    "!pip install -q langchain\n",
    "!pip show google-generativeai\n",
    "!pip show langchain-google-genai\n",
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQKwp5wpPs0j"
   },
   "source": [
    "Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "5mF-6f4lHPss",
    "outputId": "ed778d63-23b6-4cc1-f6c1-3384abe8cc1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "genai.configure(api_key='your API Key')\n",
    "\n",
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G5t4gjm4HKU9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_API_KEY']= 'Your-API-Key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZLBa-tAIcCU",
    "outputId": "469c7f0d-a1db-43e1-89c2-e4b86d73469a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the year 2099, life on the moon has become a reality. The once barren and desolate landscape has been transformed into a thriving lunar colony, home to a diverse community of scientists, engineers, and explorers. The colony is powered by advanced solar and nuclear energy systems, providing a sustainable source of electricity. Habitation modules, constructed from lightweight and durable materials, offer comfortable living spaces with artificial gravity and breathable air. Lunar greenhouses cultivate a variety of crops, providing fresh produce for the colony's inhabitants. Rovers and lunar landers facilitate exploration and research, allowing scientists to study the moon's geology, atmosphere, and potential resources. The colony also serves as a hub for space exploration, with missions to Mars and beyond being launched from its lunar base.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm= ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)\n",
    "response= llm.invoke('Write a paraghraph about life on moon in the year 2099')\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CXtcUysP7EP"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7W3oowlcP5p6"
   },
   "source": [
    "creating function for print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R1Ht4pSzJFeL"
   },
   "outputs": [],
   "source": [
    "def my_print(output):\n",
    "  output_str= str(output)\n",
    "  import textwrap\n",
    "  print(textwrap.fill(output_str, width=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03wQ1rYZQGgA"
   },
   "source": [
    "Prompt example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM4MZJ78Jlhi",
    "outputId": "c94c3af2-ba58-4b2e-90f1-1f17e92fe8a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplat\n",
      "e(input_variables=['\n",
      "topic'],\n",
      "template='You are\n",
      "content reater.\n",
      "Write me a tweet\n",
      "about {topic}') llm=\n",
      "ChatGoogleGenerative\n",
      "AI(model='models/gem\n",
      "ini-pro',\n",
      "temperature=0.0, cli\n",
      "ent=<google.ai.gener\n",
      "ativelanguage_v1beta\n",
      ".services.generative\n",
      "_service.client.Gene\n",
      "rativeServiceClient\n",
      "object at\n",
      "0x7b85e055da80>, asy\n",
      "nc_client=<google.ai\n",
      ".generativelanguage_\n",
      "v1beta.services.gene\n",
      "rative_service.async\n",
      "_client.GenerativeSe\n",
      "rviceAsyncClient\n",
      "object at\n",
      "0x7b85e043cb20>,\n",
      "default_metadata=())\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "my_llm= ChatGoogleGenerativeAI(model='gemini-pro', temperature=0)\n",
    "\n",
    "my_prompt= PromptTemplate.from_template('You are content reater. Write me a tweet about {topic}')\n",
    "\n",
    "chain = LLMChain(llm= my_llm,\n",
    "                prompt= my_prompt,\n",
    "                )\n",
    "topic=\"why will AI change the world\"\n",
    "\n",
    "my_print(chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_TLGXisLPH_",
    "outputId": "c8f67b7a-e41b-479c-e8a8-09a5fa3a2f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'why will\n",
      "AI change the\n",
      "world', 'text': \"AI\n",
      "is poised to\n",
      "revolutionize our\n",
      "world, transforming\n",
      "industries,\n",
      "enhancing human\n",
      "capabilities, and\n",
      "unlocking\n",
      "unprecedented\n",
      "possibilities. From\n",
      "healthcare\n",
      "advancements to\n",
      "automated processes,\n",
      "AI's impact will be\n",
      "profound and far-\n",
      "reaching.\n",
      "#AIRevolution\n",
      "#FutureTech\"}\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke(input=topic)\n",
    "my_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v16EekTwLkF6",
    "outputId": "0049c55b-ba0a-42d9-fb8c-8c8de639bbd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI is poised to\n",
      "revolutionize our\n",
      "world, transforming\n",
      "industries,\n",
      "enhancing human\n",
      "capabilities, and\n",
      "unlocking\n",
      "unprecedented\n",
      "possibilities. From\n",
      "healthcare\n",
      "advancements to\n",
      "automated processes,\n",
      "AI's impact will be\n",
      "profound and far-\n",
      "reaching.\n",
      "#AIRevolution\n",
      "#FutureTech\n"
     ]
    }
   ],
   "source": [
    "my_print(response['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1SzgTrwQNeV"
   },
   "source": [
    "Prompt example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWBFWv2WLyVE",
    "outputId": "7c4f2c00-8a01-425a-f756-b14e048eafd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "my_llm= ChatGoogleGenerativeAI(model='gemini-pro', convert_system_message_to_human= True)\n",
    "\n",
    "output=my_llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content='Answer only YES or NO'),\n",
    "        HumanMessage(content='Is shark a mammel?')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PERyzzyfQRud"
   },
   "source": [
    "Prompt example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6TR1-TwJMtSL"
   },
   "outputs": [],
   "source": [
    "prompt_template_text = \"\"\"You are a high school history teacher grading homework assignments. \\\n",
    "Based on the homework question indicated by “**Q:**” and the correct answer indicated by “**A:**”, your task is to determine whether the student's answer is correct. \\\n",
    "Grading is binary; therefore, student answers can be correct or wrong. \\\n",
    "Simple misspellings are okay.\n",
    "\n",
    "**Q:** {question}\n",
    "**A:** {correct_answer}\n",
    "\n",
    "**Student's Answer:** {student_answer}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"correct_answer\", \"student_answer\"], template = prompt_template_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bfDTvk4cNL5u"
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(\n",
    "    llm=my_llm,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "Gr2RaVq9NS3D",
    "outputId": "fecbae61-cb6f-42b3-f1f9-1cb91ed809d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Correct'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define inputs\n",
    "question = \"Who was the 35th president of the United States of America?\"\n",
    "correct_answer = \"John F. Kennedy\"\n",
    "student_answer =  \"JFK\"\n",
    "\n",
    "# run chain\n",
    "chain.run({'question':question, 'correct_answer':correct_answer, 'student_answer':student_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80zMTOV7NZp5",
    "outputId": "f046cb1a-2ac9-4ec4-ab63-112140915dee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John F. Kennedy - Correct\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JFK - Correct\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDR - Wrong\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John F. Kenedy - Correct\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Kennedy - Correct\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack Kennedy - Correct\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacqueline Kennedy - Wrong\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert F. Kenedy - Wrong\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run chain in for loop\n",
    "student_answer_list = [\"John F. Kennedy\", \"JFK\", \"FDR\", \"John F. Kenedy\", \"John Kennedy\", \"Jack Kennedy\", \"Jacqueline Kennedy\", \"Robert F. Kenedy\"]\n",
    "\n",
    "for student_answer in student_answer_list:\n",
    "    print(student_answer + \" - \" + str(chain.run({'question':question, 'correct_answer':correct_answer, 'student_answer':student_answer})))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCzvuv_hQits"
   },
   "source": [
    "Prompt example 4 --Language Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "X9zKGXiIOwUN",
    "outputId": "7407bdc4-cd16-462b-9fa4-0b7d8bc01e9d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"In an easy way translate the following sentence 'How are you' into hindi\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template='''In an easy way translate the following sentence '{sentence}' into {target_language}'''\n",
    "language_prompt = PromptTemplate(\n",
    "    input_variables=[\"sentence\",'target_language'],\n",
    "    template=template,\n",
    ")\n",
    "language_prompt.format(sentence=\"How are you\",target_language='hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyB0QrWBO0X1",
    "outputId": "ac5a2950-fcae-4e02-9ac7-1cf02e369157"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': 'Hello How are you',\n",
       " 'target_language': 'hindi',\n",
       " 'text': 'नमस्कार आप कैसे हैं'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2=LLMChain(llm=my_llm,prompt=language_prompt)\n",
    "\n",
    "chain2({'sentence':\"Hello How are you\",'target_language':'hindi'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNDBS5eAQ-BD"
   },
   "source": [
    "Prompt example 5 --Few shot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Q6Djq4JgO7sk"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "example_formatter_template = \"\"\"Word: {word}\n",
    "Antonym: {antonym}\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LNJRK8ilPAeB"
   },
   "outputs": [],
   "source": [
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\\n\",\n",
    "    suffix=\"Word: {input}\\nAntonym: \",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcfsvlduPD04",
    "outputId": "01427a5b-6954-4089-b9c5-ebcff1e60a84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: big\n",
      "Antonym: \n"
     ]
    }
   ],
   "source": [
    "print(few_shot_prompt.format(input='big'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyMIJdM3PHKq",
    "outputId": "4b7a2f7f-d67e-4dc6-9452-f6cdd7997f32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_google_genai/chat_models.py:345: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'big', 'text': 'small'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3=LLMChain(llm=my_llm,prompt=few_shot_prompt)\n",
    "chain3({'input':\"big\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAyONW-EPQMz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03o8EvogJOuP"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
